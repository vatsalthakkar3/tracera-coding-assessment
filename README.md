# TRACERA: ESG Data Extractor

This project is an automated data extraction pipeline designed to parse PDF utility bills, extract key information using LLMs, and save the structured data into a CSV file. It leverages LlamaParse for high-fidelity document-to-markdown conversion and supports both Gemini and OpenAI models for the core extraction logic.

## Features

- **Advanced PDF Parsing**: Utilizes LlamaParse for robust, OCR-powered parsing of PDF documents into a clean markdown format.
- **Intelligent Data Extraction**: Employs LLMs (configurable for Gemini or OpenAI) to accurately extract predefined fields from unstructured text.
- **Data Consolidation**: Includes a smart consolidation step to merge and de-duplicate records extracted from different parts of a single document, ensuring data integrity.
- **Efficient Caching**: Caches parsed document content to significantly speed up subsequent processing runs.
- **Structured Output**: Saves the final, cleaned data to a CSV file in the `output/` directory.
- **Streamlined Workflow**: Comes with a `Makefile` providing simple commands for setup, execution, testing, and code quality checks.

## Project Structure

```
.
├── data/                  # Input directory for source PDF files.
├── output/                # Output directory for the extracted CSV data.
├── cache/                 # Caches parsed markdown files to accelerate reruns.
├── src/                   # Main source code.
│   ├── config.py          # Project configuration (API keys, paths, models).
│   ├── main.py            # Main entry point for the application.
│   ├── schemas.py         # Pydantic models for structured data.
│   └── utils/             # Core utility modules.
│       ├── data_extractor.py # Orchestrates the extraction process.
│       ├── file_handler.py   # Handles file I/O (reading PDFs, saving CSV).
│       ├── llm_service.py    # Manages interaction with the LLM APIs.
│       └── pdf_parser.py     # Handles PDF parsing using LlamaParse.
├── tests/                 # Unit and integration tests.
├── Makefile               # Commands for running, testing, and formatting.
├── pyproject.toml         # Project metadata and dependencies.
├── requirements.txt       # Pinned Python dependencies generated by uv.
└── README.md              # This file.
```

## Setup and Installation

### Prerequisites

- **Python 3.12**: The project requires Python 3.12, as specified in the `.python-version` file.
- **uv**: This project uses `uv` for fast Python package management.

  You can install `uv` using the official installers:
  - **macOS / Linux**:
    ```sh
    curl -LsSf https://astral.sh/uv/install.sh | sh
    ```
  - **Windows**:
    ```sh
    powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
    ```
  - Alternatively, you can install it via `pip`:
    ```sh
    pip install uv
    ```

### Installation Steps

1.  **Clone the repository:**
    ```sh
    git clone <repository-url>
    cd tracera-coding-assessment
    ```

2.  **Create a virtual environment:**
    ```sh
    uv venv --python 3.12
    source .venv/bin/activate
    ```

3.  **Install dependencies:**

    - **Using `uv` (Recommended):**
      The `sync` command ensures your environment matches the lock file exactly.
      ```sh
      # Install main and development dependencies
      uv pip sync
      ```

    - **Using `pip`:**
      If you prefer not to use `uv`, you can use pip:
      ```sh
      pip install -r requirements.txt
      ```

## Environment Variables

You need to configure API keys for the services used in this project.

1.  **Create a `.env` file** by copying the example file:
    ```sh
    cp .env.example .env
    ```

2.  **Edit the `.env` file** and add your API keys:
    - `LLAMA_CLOUD_API_KEY`: **(Required)** For parsing PDFs with LlamaParse.
    - `GEMINI_API_KEY` or `OPENAI_API_KEY`: **(Required)** You must provide at least one of these for the data extraction LLM. The project prioritizes Gemini if both are set.

    ```env
    # .env
    GEMINI_API_KEY="AI-..."
    OPENAI_API_KEY="sk-..."
    LLAMA_CLOUD_API_KEY="llx-..."
    ```

## Usage

The primary way to run the data extraction pipeline is by using the `make run` command.

1.  **Place your PDF files** into the `data/` directory.
2.  **Run the extraction process:**
    ```sh
    make run
    ```
    This command will process all PDF files in the `data` directory, extract the relevant information, and save the results to `output/extracted_data.csv`.

## Running with Docker

This project is fully containerized, allowing you to build and run it using Docker without needing to manage Python environments locally.

### Prerequisites

- [Docker](https://docs.docker.com/get-docker/) installed and running on your system.

### Build the Docker Image

From the project root directory, run the following command to build the Docker image:

```sh
docker build --no-cache -t tracera-extractor .
```

### Run the Container

After building the image, you can run the data extraction pipeline inside a container. Make sure your `.env` file is populated with the required API keys.

```sh
docker run --rm --env-file .env -v ./data:/app/data -v ./output:/app/output -v ./cache:/app/cache tracera-extractor
```

#### Command Explanation:

- `docker run`: The command to start a new container.
- `--rm`: Automatically removes the container when it exits, keeping your system clean.
- `--env-file .env`: Passes the environment variables from your local `.env` file into the container.
- `-v ./data:/app/data`: Mounts the local `data` directory (containing input PDFs) into the container's `/app/data` directory.
- `-v ./output:/app/output`: Mounts the local `output` directory into the container. The application will write the final `extracted_data.csv` here.
- `-v ./cache:/app/cache`: Mounts the local `cache` directory to persist parsed documents between runs, improving performance.
- `tracera-extractor`: The name of the image to run.

## Makefile Commands

The project includes a `Makefile` with several commands to streamline development and execution:

- `make run`: Executes the main data extraction pipeline.
- `make test`: Runs the entire test suite using `pytest`.
- `make coverage`: Runs tests and generates a detailed coverage report in the terminal and as an HTML report in `htmlcov/`.
- `make format`: Formats the codebase using `ruff format`.
- `make lint`: Lints the code for style and errors using `ruff check`.
- `make clean`: Removes temporary files and caches, such as `__pycache__`, `.pytest_cache`, and `.coverage`.
